{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":489960,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":389753,"modelId":408584},{"sourceId":489957,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":389751,"modelId":408582}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Very important\n# Face Unlock Application Access Instructions\n\n# \n# To access the Streamlit Face Unlock Application:\n# 1.  **Run this entire Kaggle Notebook from top to bottom.**\n#     Ensure all cells execute successfully.\n#  2.  **Wait patiently during the \"Phase 3: Running Streamlit App\" section.**\n#     You will see continuous \"--- Current Streamlit Log Tail\" messages.\n#     This indicates the application is starting and loading models.\n#     This process can take 1-3 minutes depending on the Kaggle instance.\n# 3.  **Look for the `ngrok` Public URL.**\n#     Once Streamlit is fully launched, the notebook will print the `ngrok` tunnel URL,\n#     which will look something like:\n#     `Your ngrok URL is: https://<random-characters>.ngrok-free.app`\n#  4.  **Click on this `ngrok` URL.**\n#     This is the ONLY link that will work to access your Streamlit app from outside Kaggle.\n#     (Do NOT use `localhost` or internal IP addresses provided in Streamlit's own logs.)\n#  5.  **Keep the Kaggle Notebook cell running.**\n#     The cell will continuously print a message like \"[HH:MM:SS] Kaggle cell active...\"\n#     **You MUST keep this cell running for the Streamlit app to remain online and accessible.**\n#     If you stop the cell, the app will go offline immediately.\n# 6.  **To stop the application and release resources:**\n#     Simply click the \"Stop\" button (red square) on the top-right of this Kaggle cell.\n#     The notebook will then perform a clean shutdown.\n","metadata":{"execution":{"iopub.status.busy":"2025-07-30T22:33:53.293522Z","iopub.execute_input":"2025-07-30T22:33:53.294274Z","iopub.status.idle":"2025-07-30T22:33:53.298055Z","shell.execute_reply.started":"2025-07-30T22:33:53.294225Z","shell.execute_reply":"2025-07-30T22:33:53.297435Z"}}},{"cell_type":"code","source":"import os\nimport shutil\nimport time\nfrom subprocess import Popen, PIPE, STDOUT\nimport sys\nimport glob\nimport gc # Import garbage collector\nimport tensorflow as tf # Import tensorflow here for memory config\nimport threading # Still need threading for ngrok tunnel\nimport select # For checking if pipe has data (though less relevant with nohup direct logging)\n\n# --- Cleanup Step: Terminate previous processes ---\nprint(\"--- Cleanup: Shutting down old processes... ---\")\n# Use killall for a more aggressive cleanup of any lingering Streamlit/ngrok processes\n# The `|| true` prevents errors if the process isn't found\n!fuser -k -9 8501/tcp || true\n!pkill -f ngrok || true\n!killall -q streamlit || true # Ensure any old streamlit processes are gone\nprint(\"Cleanup complete. Starting new session.\")\n\n# --- Step 0: Setup ---\nprint(\"\\n--- Setup: Installing/Upgrading dependencies. This will take some time. ---\")\n# Prioritize your core dependencies.\n!{sys.executable} -m pip install -q --upgrade \\\n    streamlit==1.36.0 \\\n    opencv-python-headless \\\n    numpy \\\n    tensorflow \\\n    Pillow \\\n    h5py \\\n    scikit-learn \\\n    av \\\n    requests \\\n    tqdm \\\n    pyngrok\n\nprint(\"Dependency installation/upgrade attempt complete.\")\n\n# --- Configure TensorFlow Memory Growth ---\n# This helps prevent OOM errors by allocating GPU memory as needed\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"TensorFlow: Memory growth enabled for GPUs.\")\n    except RuntimeError as e:\n        print(f\"TensorFlow: Could not set memory growth: {e}\")\nelse:\n    print(\"TensorFlow: No GPU devices found or configured.\")\n\n# Clear TensorFlow session and collect garbage to free up memory\ntf.keras.backend.clear_session()\ngc.collect()\n\n# --- Verify pyngrok installation ---\nprint(\"Verifying pyngrok installation...\")\ntry:\n    from pyngrok import ngrok\n    print(\"pyngrok imported successfully.\")\nexcept ImportError:\n    print(\"Error: pyngrok could not be imported. Please ensure it installed correctly.\")\n    exit()\n\n# --- Step 1: Define Paths and Prepare Files ---\nprint(\"\\n--- Step 1: Preparing files for app launch ---\")\n\n# Define base paths for artifacts and app\n# !!! IMPORTANT: YOU MUST CORRECT THIS PATH BASED ON YOUR KAGGLE DATASET LOCATION !!!\n# Use `!ls -R /kaggle/input/` in a separate cell on Kaggle to find the exact path.\n# Example: If your dataset is named 'my-face-models' and files are at its root,\n# ARTIFACTS_INPUT_PATH = '/kaggle/input/my-face-models/'\nARTIFACTS_INPUT_PATH = '/kaggle/input/lma/pytorch/default/1/' # <--- REVIEW AND CORRECT THIS LINE\n\nAPP_ROOT_KAGGLE = '/kaggle/working/face_unlock_app/'\nMODELS_DIR_KAGGLE = os.path.join(APP_ROOT_KAGGLE, 'models')\nDNN_MODELS_DIR = os.path.join(MODELS_DIR_KAGGLE, 'dnn_face_detector_models')\nDB_PATH = os.path.join(APP_ROOT_KAGGLE, 'face_db.db')\n\n# Add a cleanup step for the database file\nprint(\"Cleaning up old database file...\")\ntry:\n    if os.path.exists(DB_PATH):\n        os.remove(DB_PATH)\n        print(f\"Old database file '{DB_PATH}' removed successfully.\")\n    else:\n        print(\"No old database file found. Starting fresh.\")\nexcept OSError as e:\n    print(f\"Warning: Could not remove database file. The app will attempt to fix the schema instead.\")\n\n# Create destination directories\nos.makedirs(APP_ROOT_KAGGLE, exist_ok=True)\nos.makedirs(MODELS_DIR_KAGGLE, exist_ok=True)\nos.makedirs(DNN_MODELS_DIR, exist_ok=True)\n\n# Define full, absolute paths for all files\nLIVENESS_MODEL_SRC = os.path.join(ARTIFACTS_INPUT_PATH, 'liveness_model.h5')\nLIVENESS_MODEL_DST = os.path.join(MODELS_DIR_KAGGLE, 'liveness_model.h5')\n\nFACE_EMBEDDING_MODEL_SRC = os.path.join(ARTIFACTS_INPUT_PATH, 'face_embedding_model.h5')\nFACE_EMBEDDING_MODEL_DST = os.path.join(MODELS_DIR_KAGGLE, 'face_embedding_model.h5')\n\nFACE_DB_SRC = os.path.join(ARTIFACTS_INPUT_PATH, 'face_db.db')\nFACE_DB_DST = os.path.join(APP_ROOT_KAGGLE, 'face_db.db')\n\nCAFFE_MODEL_SRC = os.path.join(ARTIFACTS_INPUT_PATH, 'res10_300x300_ssd_iter_140000_fp16.caffemodel')\nCAFFE_MODEL_DST = os.path.join(DNN_MODELS_DIR, 'res10_300x300_ssd_iter_140000_fp16.caffemodel')\n\nPROTOTXT_SRC = os.path.join(ARTIFACTS_INPUT_PATH, 'deploy.prototxt')\nPROTOTXT_DST = os.path.join(DNN_MODELS_DIR, 'deploy.prototxt')\n\n# Copy artifacts to their destinations\ntry:\n    print(f\"Attempting to copy liveness model from: {LIVENESS_MODEL_SRC} to {LIVENESS_MODEL_DST}\")\n    shutil.copy(LIVENESS_MODEL_SRC, LIVENESS_MODEL_DST)\n    print(f\"Attempting to copy face embedding model from: {FACE_EMBEDDING_MODEL_SRC} to {FACE_EMBEDDING_MODEL_DST}\")\n    shutil.copy(FACE_EMBEDDING_MODEL_SRC, FACE_EMBEDDING_MODEL_DST)\n    print(f\"Attempting to copy face detector caffemodel from: {CAFFE_MODEL_SRC} to {CAFFE_MODEL_DST}\")\n    shutil.copy(CAFFE_MODEL_SRC, CAFFE_MODEL_DST)\n    print(f\"Attempting to copy face detector prototxt from: {PROTOTXT_SRC} to {PROTOTXT_DST}\")\n    shutil.copy(PROTOTXT_SRC, PROTOTXT_DST)\n    \n    if os.path.exists(FACE_DB_SRC):\n        print(f\"Attempting to copy existing database from: {FACE_DB_SRC} to {FACE_DB_DST}\")\n        shutil.copy(FACE_DB_SRC, FACE_DB_DST)\n    else:\n        print(f\"Database source file not found at {FACE_DB_SRC}. A new one will be created.\")\n    \n    print(\"All necessary files attempted to be copied.\")\n\nexcept FileNotFoundError as e:\n    print(f\"Error: A required file was not found during copy. Please check your source paths.\")\n    print(f\"Details: {e}\")\n    # Exit if critical files cannot be found at source\n    exit()\nexcept Exception as e:\n    print(f\"An unexpected error occurred during file copying: {e}\")\n    exit()\n\n# --- Verification Step ---\nprint(\"Verifying model files exist in destination...\")\nrequired_files = [LIVENESS_MODEL_DST, CAFFE_MODEL_DST, PROTOTXT_DST]\nall_found = True\nfor f in required_files:\n    if not os.path.exists(f):\n        print(f\"ERROR: Required file not found in destination: {f}\")\n        all_found = False\n    else:\n        print(f\"Verified: {f} exists.\")\nif not all_found:\n    print(\"Some required model files are missing. Please fix input paths or dataset attachments.\")\n    exit()\nprint(\"All required model files verified in destination.\")\n\n\n# --- Step 2: Generating Python files ---\nprint(\"\\n--- Step 2: Generating Python files ---\")\n\n# We'll generate utils.py and app.py using full absolute paths\nmodels_abs_path = os.path.abspath(MODELS_DIR_KAGGLE)\napp_root_abs_path = os.path.abspath(APP_ROOT_KAGGLE)\n\nutils_code = f\"\"\"\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_v2_preprocess\nimport h5py\nimport sqlite3\nimport streamlit as st\nimport datetime\nimport gc # Import garbage collector\n\n# Clear TensorFlow session to free up memory (redundant if already cleared in main script, but safe)\ntf.keras.backend.clear_session()\ngc.collect()\n\n# Define model paths using the absolute paths passed from the main script\nMODELS_PATH_GLOBAL = \"{models_abs_path}\"\n\n# Database file path using absolute path\nDB_PATH = \"{os.path.abspath(os.path.join(app_root_abs_path, 'face_db.db'))}\"\n\n# Database Manager Class\nclass SQLiteDatabaseManager:\n    def __init__(self, db_path=DB_PATH):\n        self.db_path = db_path\n        self._initialize_db()\n\n    def _initialize_db(self):\n        conn = self._get_connection()\n        c = conn.cursor()\n\n        c.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='users'\")\n        table_exists = c.fetchone()\n\n        if table_exists:\n            c.execute(\"PRAGMA table_info(users)\")\n            columns = [col[1] for col in c.fetchall()]\n            if 'registration_date' not in columns:\n                print(\"Adding missing 'registration_date' column to 'users' table.\")\n                c.execute(\"ALTER TABLE users ADD COLUMN registration_date TEXT\")\n        else:\n            c.execute('''\n                CREATE TABLE users (\n                    id INTEGER PRIMARY KEY,\n                    username TEXT UNIQUE NOT NULL,\n                    embedding BLOB NOT NULL,\n                    registration_date TEXT\n                )\n            ''')\n\n        c.execute('''\n            CREATE TABLE IF NOT EXISTS history (\n                id INTEGER PRIMARY KEY,\n                username TEXT,\n                event_type TEXT,\n                timestamp TEXT,\n                success INTEGER\n            )\n        ''')\n        conn.commit()\n        conn.close()\n\n    def _get_connection(self):\n        return sqlite3.connect(self.db_path)\n\n    def register_user(self, username, embedding):\n        try:\n            conn = self._get_connection()\n            c = conn.cursor()\n            embedding_blob = sqlite3.Binary(embedding)\n            registration_date = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n            c.execute(\"INSERT INTO users (username, embedding, registration_date) VALUES (?, ?, ?)\", \n                      (username, embedding_blob, registration_date))\n            conn.commit()\n            conn.close()\n            return True, \"User registered successfully.\"\n        except sqlite3.IntegrityError:\n            return False, \"Username already exists.\"\n        except Exception as e:\n            return False, f\"Database error: {{e}}\"\n\n    def get_user_embedding(self, username):\n        conn = self._get_connection()\n        c = conn.cursor()\n        c.execute(\"SELECT embedding FROM users WHERE username = ?\", (username,))\n        result = c.fetchone()\n        conn.close()\n        if result:\n            return np.frombuffer(result[0], dtype=np.float32)\n        return None\n\n    def get_all_users(self):\n        conn = self._get_connection()\n        c = conn.cursor()\n        c.execute(\"SELECT username, registration_date FROM users\")\n        users = c.fetchall()\n        conn.close()\n        return users\n    \n    def log_event(self, username, event_type, success):\n        conn = self._get_connection()\n        c = conn.cursor()\n        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        c.execute(\"INSERT INTO history (username, event_type, timestamp, success) VALUES (?, ?, ?, ?)\", \n                  (username, event_type, timestamp, int(success)))\n        conn.commit()\n        conn.close()\n\n    def get_history(self, username=None):\n        conn = self._get_connection()\n        c = conn.cursor()\n        if username:\n            c.execute(\"SELECT username, event_type, timestamp, success FROM history WHERE username = ?\", (username,))\n        else:\n            c.execute(\"SELECT username, event_type, timestamp, success FROM history\")\n        history = c.fetchall()\n        conn.close()\n        return history\n\n# Model and Detector Loading Functions using absolute paths\n@st.cache_resource\ndef load_face_embedding_model():\n    # *** FIX: Changed alpha to 0.35 as 0.25 is not available with imagenet weights ***\n    # This should resolve the ValueError and is the smallest pre-trained model.\n    base_model = MobileNetV2(\n        input_shape=(224, 224, 3), \n        include_top=False, \n        weights='imagenet', # Retaining ImageNet weights\n        alpha=0.35 # Changed from 0.25 to 0.35 (smallest available with imagenet)\n    )\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    embedding_model = Model(inputs=base_model.input, outputs=x)\n    return embedding_model\n\n@st.cache_resource\ndef load_liveness_model():\n    return None \n\n@st.cache_resource\ndef load_face_detector(prototxt_path, model_path):\n    return cv2.dnn.readNetFromCaffe(prototxt_path, model_path)\n\n# Core ML Logic Functions\ndef detect_and_align_face(frame, net):\n    (h, w) = frame.shape[:2]\n    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n    net.setInput(blob)\n    detections = net.forward()\n    for i in range(0, detections.shape[2]):\n        confidence = detections[0, 0, i, 2]\n        if confidence > 0.5:\n            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n            (startX, startY, endX, endY) = box.astype(\"int\")\n            \n            face_width = endX - startX\n            face_height = endY - startY\n            aspect_ratio = face_width / face_height\n            if not (0.7 < aspect_ratio < 1.3): \n                return None, None\n\n            face = frame[startY:endY, startX:endX]\n            if face.shape[0] == 0 or face.shape[1] == 0: \n                continue\n            face = cv2.resize(face, (224, 224))\n            return face, (startX, startY, endX, endY)\n    return None, None\n\ndef get_face_embedding(face, embedding_model):\n    face = face.astype('float32')\n    face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n    face_processed = mobilenet_v2_preprocess(face_rgb)\n    face_expanded = np.expand_dims(face_processed, axis=0)\n    embedding = embedding_model.predict(face_expanded, verbose=0)[0]\n    embedding = embedding / np.linalg.norm(embedding)\n    return embedding\n\ndef check_liveness(face_image):\n    gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    \n    _, thresh = cv2.threshold(blurred, 220, 255, cv2.THRESH_BINARY) \n    \n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    if contours:\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area > 50 and area < 500: \n                (x, y), radius = cv2.minEnclosingCircle(contour)\n                if cv2.arcLength(contour, True) > 0:\n                    circularity = (4 * np.pi * area) / (cv2.arcLength(contour, True) ** 2)\n                    if circularity > 0.6: \n                        return False, \"Suspicious reflection detected. Please remove any shiny objects.\"\n    return True, \"Liveness check passed.\"\n    \ndef verify_face(known_embedding, new_embedding, threshold=0.5):\n    distance = np.linalg.norm(known_embedding - new_embedding)\n    is_match = distance < threshold\n    return is_match, distance\n\"\"\"\n\napp_code = f\"\"\"\nimport streamlit as st\nimport cv2\nimport numpy as np\nfrom utils import (\n    load_face_embedding_model, \n    load_face_detector, \n    check_liveness,\n    verify_face,\n    SQLiteDatabaseManager,\n    detect_and_align_face,\n    get_face_embedding\n)\nimport os\n\n# Define model paths using the absolute paths passed from the main script\nMODELS_DIR = \"{models_abs_path}\"\nDNN_MODELS_DIR = \"{os.path.join(models_abs_path, 'dnn_face_detector_models')}\"\n\n# Load models and database manager only once using Streamlit's caching\nface_embedding_model = load_face_embedding_model()\nface_detector = load_face_detector(os.path.join(DNN_MODELS_DIR, 'deploy.prototxt'), os.path.join(DNN_MODELS_DIR, 'res10_300x300_ssd_iter_140000_fp16.caffemodel'))\ndb_manager = SQLiteDatabaseManager()\n\nif 'page' not in st.session_state:\n    st.session_state.page = 'Home'\nif 'live_frame' not in st.session_state:\n    st.session_state.live_frame = None\nif 'username' not in st.session_state:\n    st.session_state.username = ''\n\nst.sidebar.title(\"Navigation\")\npage = st.sidebar.radio(\"Go to\", [\"Home\", \"Register\", \"Authenticate\", \"History\"])\n\nif page == \"Home\":\n    st.title(\"Face Unlock System\")\n    st.write(\"Welcome to the face unlock application. Use the sidebar to navigate.\")\n    st.info(\"This application supports webcam authentication and registration, as well as a history log.\")\n\nelif page == \"Register\":\n    st.title(\"User Registration\")\n    st.info(\"Enter a username and use your webcam to take a photo. A clear, well-lit photo works best.\")\n    \n    username = st.text_input(\"Enter a username to register:\")\n    st.session_state.username = username\n    \n    registration_mode = st.radio(\"Choose registration method:\", [\"Webcam\", \"Upload Photo\"])\n    \n    if registration_mode == \"Webcam\":\n        captured_photo = st.camera_input(\"Take a picture to register\")\n        if captured_photo is not None:\n            if not username:\n                st.error(\"Please enter a username.\")\n            else:\n                file_bytes = np.asarray(bytearray(captured_photo.read()), dtype=np.uint8)\n                img = cv2.imdecode(file_bytes, 1)\n\n                face, box = detect_and_align_face(img, face_detector)\n                if face is not None:\n                    embedding = get_face_embedding(face, face_embedding_model)\n                    success, message = db_manager.register_user(username, embedding.tobytes())\n                    if success:\n                        st.success(f\"Registration successful for user '{{username}}'!\")\n                    else:\n                        st.error(message)\n                else:\n                    st.warning(\"No face detected in the captured image. Please try again.\")\n    \n    elif registration_mode == \"Upload Photo\":\n        uploaded_file = st.file_uploader(\"Upload a photo to register\", type=[\"jpg\", \"jpeg\", \"png\"])\n        if uploaded_file is not None:\n            if not username:\n                st.error(\"Please enter a username.\")\n            else:\n                file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n                img = cv2.imdecode(file_bytes, 1)\n                st.image(img, channels=\"BGR\", caption=\"Uploaded Photo\")\n\n                face, box = detect_and_align_face(img, face_detector)\n                if face is not None:\n                    embedding = get_face_embedding(face, face_embedding_model)\n                    success, message = db_manager.register_user(username, embedding.tobytes())\n                    if success:\n                        st.success(f\"Registration successful for user '{{username}}'!\")\n                    else:\n                        st.error(message)\n                else:\n                    st.warning(\"No face detected in the uploaded image. Please try again.\")\n\n\nelif page == \"Authenticate\":\n    st.title(\"User Authentication\")\n    st.info(\"Authenticate using your webcam. A liveness check will be performed.\")\n\n    auth_username = st.text_input(\"Enter username for authentication:\")\n    captured_photo = st.camera_input(\"Take a picture to authenticate\")\n\n    if captured_photo is not None:\n        if not auth_username:\n            st.error(\"Please enter a username.\")\n        else:\n            stored_embedding_bytes = db_manager.get_user_embedding(auth_username)\n            if stored_embedding_bytes is None:\n                st.error(f\"User '{{auth_username}}' not found.\")\n                db_manager.log_event(auth_username, \"Authentication\", False)\n            else:\n                file_bytes = np.asarray(bytearray(captured_photo.read()), dtype=np.uint8)\n                live_frame = cv2.imdecode(file_bytes, 1)\n\n                face, box = detect_and_align_face(live_frame, face_detector)\n                if face is not None:\n                    is_live, liveness_message = check_liveness(live_frame)\n                    if is_live:\n                        live_embedding = get_face_embedding(face, face_embedding_model)\n                        is_match, distance = verify_face(stored_embedding_bytes, live_embedding)\n                        if is_match:\n                            st.success(f\"Authentication Successful! Distance: {{distance:.2f}} (Lower is better)\")\n                            db_manager.log_event(auth_username, \"Authentication\", True)\n                        else:\n                            st.error(f\"Authentication Failed. Distance: {{distance:.2f}}\")\n                            db_manager.log_event(auth_username, \"Authentication\", False)\n                    else:\n                        st.warning(f\"Liveness check failed: {{liveness_message}}\")\n                        db_manager.log_event(auth_username, \"Authentication\", False)\n                else:\n                    st.warning(\"No face detected in the image.\")\n                    db_manager.log_event(auth_username, \"Authentication\", False)\n\nelif page == \"History\":\n    st.title(\"Activity History\")\n    \n    history = db_manager.get_history()\n    \n    if history:\n        st.write(\"---\")\n        for username, event_type, timestamp, success in history:\n            status = \"✅ Success\" if success else \"❌ Failed\"\n            st.write(f\"**User:** {{username}} | **Event:** {{event_type}} | **Timestamp:** {{timestamp}} | **Status:** {{status}}\")\n        st.write(\"---\")\n    else:\n        st.info(\"No activity history found.\")\n\n    st.write(\"---\")\n    st.header(\"Registered Users\")\n    users = db_manager.get_all_users()\n    if users:\n        for username, reg_date in users:\n            st.write(f\"**User:** {{username}} | **Registration Date:** {{reg_date}}\")\n    else:\n        st.info(\"No users registered yet.\")\n\"\"\"\n\n# Write the generated code to files\nwith open(os.path.join(APP_ROOT_KAGGLE, 'utils.py'), \"w\") as f:\n    f.write(utils_code)\nwith open(os.path.join(APP_ROOT_KAGGLE, 'app.py'), \"w\") as f:\n    f.write(app_code)\nprint(\"utils.py and app.py generated successfully.\")\n\n# --- Phase 3: Running Streamlit App with ngrok (NOHUP + External Log Monitoring) ---\nprint(\"\\n--- Phase 3: Running Streamlit App ---\")\nprint(\"Attempting to set ngrok auth token...\")\ntry:\n    # IMPORTANT: Ensure your ngrok auth token is correct and updated if needed.\n    ngrok.set_auth_token(\"30NVTR3IDN5fDUVZ6XegDSSj2ho_SCnczzHNLMG43LqgYpAK\") # Replace with your actual ngrok token\n    print(\"ngrok auth token set successfully.\")\nexcept Exception as e:\n    print(f\"Error setting ngrok auth token: {e}\")\n    print(\"Please ensure your ngrok token is correct. Exiting.\")\n    exit()\n\n# Define log file path\nlog_file_path = \"streamlit_logs.txt\"\n\n# Start Streamlit using nohup for detachment and redirect all output to a file\nprint(\"Starting Streamlit app in a detached process using nohup...\")\n# Construct the command carefully, quoting paths if they might have spaces (though not in this case)\nstreamlit_cmd_parts = [\n    \"nohup\",\n    sys.executable, \"-m\", \"streamlit\", \"run\", os.path.join(APP_ROOT_KAGGLE, 'app.py'),\n    \"--server.port\", \"8501\",\n    \"--server.enableCORS\", \"false\",\n    \"--browser.gatherUsageStats\", \"false\",\n    \"--server.headless\", \"true\",\n    \"&>\", # Redirect both stdout and stderr\n    log_file_path,\n    \"&\" # Run in background\n]\n# Join the parts with spaces to form the full shell command string\nshell_command = \" \".join(streamlit_cmd_parts)\nos.system(shell_command) # Execute as a shell command\n\nprint(f\"Streamlit app launched in background, logging to {log_file_path}\")\n\n# Now, we monitor the log file directly by tailing it\nprint(\"Monitoring Streamlit app initialization from log file...\")\nmax_wait_time = 120 # seconds, increased tolerance\ncheck_interval = 2 # seconds\nstart_time = time.time()\napp_online = False\n\nwhile time.time() - start_time < max_wait_time:\n    # Guaranteed print every check_interval\n    if os.path.exists(log_file_path):\n        try:\n            # Read from the end of the file\n            with open(log_file_path, \"r\") as f:\n                f.seek(0, os.SEEK_END)\n                fsize = f.tell()\n                f.seek(max(0, fsize - 1500), os.SEEK_SET) # Read last 1.5KB\n                tail = f.read()\n                \n                # Print the tail for a guaranteed heartbeat and recent logs\n                print(f\"\\n--- Current Streamlit Log Tail ({time.strftime('%H:%M:%S', time.localtime())}) ---\\n{tail}\\n---------------------------------------\\n\", flush=True)\n                \n                # Check for the \"You can now view...\" message in the tail\n                if \"You can now view your Streamlit app in your browser\" in tail:\n                    print(\"\\nStreamlit app detected as online in logs.\")\n                    app_online = True\n                    break\n        except Exception as e:\n            print(f\"Error reading log tail: {e}\", flush=True)\n    else:\n        print(f\"\\n--- Waiting for Streamlit Log File ({time.strftime('%H:%M:%S', time.localtime())}) ---\", flush=True) \n\n    time.sleep(check_interval)\n\nif not app_online:\n    print(\"\\n--- Streamlit app did not report readiness within the expected time. ---\")\n    print(\"It might still be initializing or have encountered an early error. Ngrok may still fail.\")\n\nprint(\"Attempting to create ngrok tunnel...\")\npublic_url = None\ntry:\n    public_url = ngrok.connect(8501)\n    print(\"ngrok tunnel created successfully.\")\n    print(f\"Your ngrok URL is: {public_url}\")\n    print(\"\\nIf the URL shows 'ERR_NGROK_3200', the Streamlit app is likely still offline or crashed. Please check the Streamlit logs.\")\nexcept Exception as e:\n    print(f\"Error starting ngrok: {e}\")\n\n# --- CRITICAL: Keep this cell running to keep the Streamlit app alive on Kaggle! ---\nprint(\"\\n--- IMPORTANT: Keep this Kaggle cell running to keep your Streamlit app alive! ---\")\nprint(f\"Access your Face Unlock app at the ngrok URL: {public_url}\")\nprint(\"To terminate the app and free resources, manually press the 'Stop' button in Kaggle.\")\n\ntry:\n    # Use a dummy loop to keep the Kaggle kernel alive.\n    # The actual Streamlit process is now detached via nohup.\n    while True:\n        # Periodically check the log file for any late errors or a crash\n        if os.path.exists(log_file_path):\n            try:\n                with open(log_file_path, \"r\") as f:\n                    f.seek(0, os.SEEK_END) # Go to end of file\n                    fsize = f.tell()\n                    # Read only the very latest tail\n                    f.seek(max(0, fsize - 200), os.SEEK_SET) \n                    latest_log_output = f.read()\n                    if \"Traceback (most recent call last):\" in latest_log_output:\n                        print(\"\\n--- Streamlit process detected a Python error or crash! ---\", flush=True)\n                        print(f\"Latest log output: {latest_log_output}\", flush=True)\n                        break # Break the loop to signal crash\n            except Exception as e:\n                print(f\"Error checking log file during runtime: {e}\", flush=True)\n        \n        time.sleep(30) # Sleep to prevent busy-waiting\n        print(f\"[{time.strftime('%H:%M:%S', time.localtime())}] Kaggle cell active, Streamlit app should be running. Check ngrok URL.\", flush=True)\nexcept KeyboardInterrupt:\n    print(\"\\nKaggle cell interrupted. Attempting cleanup...\", flush=True)\nfinally:\n    # Cleanup processes launched by the shell (nohup)\n    print(\"Performing post-interruption cleanup...\", flush=True)\n    !fuser -k -9 8501/tcp || true\n    !pkill -f ngrok || true\n    !killall -q streamlit || true # Ensure any running streamlit processes are killed\n    print(\"Cleanup commands executed.\", flush=True)\n\n    # Kill ngrok tunnel (if it was ever established)\n    if ngrok.get_tunnels(): # Check if any tunnels are active\n        print(\"Killing ngrok tunnel(s)...\", flush=True)\n        ngrok.kill()\n    else:\n        print(\"No active ngrok tunnels found to kill.\", flush=True)\n    \n    print(\"Clean shutdown complete. Resources released.\", flush=True)\n\nprint(\"\\n--- Full Streamlit Logs (after execution/interruption) ---\")\n# Ensure all logs are flushed and read at the end for final diagnosis\nif os.path.exists(log_file_path):\n    !cat {log_file_path}\nelse:\n    print(\"Streamlit log file not found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T22:27:10.018877Z","iopub.execute_input":"2025-07-30T22:27:10.019612Z","iopub.status.idle":"2025-07-30T22:31:51.994443Z","shell.execute_reply.started":"2025-07-30T22:27:10.019581Z","shell.execute_reply":"2025-07-30T22:31:51.993736Z"}},"outputs":[{"name":"stdout","text":"--- Cleanup: Shutting down old processes... ---\nCleanup complete. Starting new session.\n\n--- Setup: Installing/Upgrading dependencies. This will take some time. ---\nDependency installation/upgrade attempt complete.\nTensorFlow: Memory growth enabled for GPUs.\nVerifying pyngrok installation...\npyngrok imported successfully.\n\n--- Step 1: Preparing files for app launch ---\nCleaning up old database file...\nOld database file '/kaggle/working/face_unlock_app/face_db.db' removed successfully.\nAttempting to copy liveness model from: /kaggle/input/lma/pytorch/default/1/liveness_model.h5 to /kaggle/working/face_unlock_app/models/liveness_model.h5\nAttempting to copy face embedding model from: /kaggle/input/lma/pytorch/default/1/face_embedding_model.h5 to /kaggle/working/face_unlock_app/models/face_embedding_model.h5\nAttempting to copy face detector caffemodel from: /kaggle/input/lma/pytorch/default/1/res10_300x300_ssd_iter_140000_fp16.caffemodel to /kaggle/working/face_unlock_app/models/dnn_face_detector_models/res10_300x300_ssd_iter_140000_fp16.caffemodel\nAttempting to copy face detector prototxt from: /kaggle/input/lma/pytorch/default/1/deploy.prototxt to /kaggle/working/face_unlock_app/models/dnn_face_detector_models/deploy.prototxt\nAttempting to copy existing database from: /kaggle/input/lma/pytorch/default/1/face_db.db to /kaggle/working/face_unlock_app/face_db.db\nAll necessary files attempted to be copied.\nVerifying model files exist in destination...\nVerified: /kaggle/working/face_unlock_app/models/liveness_model.h5 exists.\nVerified: /kaggle/working/face_unlock_app/models/dnn_face_detector_models/res10_300x300_ssd_iter_140000_fp16.caffemodel exists.\nVerified: /kaggle/working/face_unlock_app/models/dnn_face_detector_models/deploy.prototxt exists.\nAll required model files verified in destination.\n\n--- Step 2: Generating Python files ---\nutils.py and app.py generated successfully.\n\n--- Phase 3: Running Streamlit App ---\nAttempting to set ngrok auth token...\nngrok auth token set successfully.\nStarting Streamlit app in a detached process using nohup...\nStreamlit app launched in background, logging to streamlit_logs.txt\nMonitoring Streamlit app initialization from log file...\n\n--- Current Streamlit Log Tail (22:27:16) ---\n\n---------------------------------------\n\n","output_type":"stream"},{"name":"stderr","text":"2025-07-30 22:27:17.376 \nWarning: the config option 'server.enableCORS=false' is not compatible with 'server.enableXsrfProtection=true'.\nAs a result, 'server.enableCORS' is being overridden to 'true'.\n\nMore information:\nIn order to protect against CSRF attacks, we send a cookie with each request.\nTo do so, we must specify allowable origins, which places a restriction on\ncross-origin resource sharing.\n\nIf cross origin resource sharing is required, please disable server.enableXsrfProtection.\n            \n","output_type":"stream"},{"name":"stdout","text":"\n  You can now view your Streamlit app in your browser.\n\n  Local URL: http://localhost:8501\n  Network URL: http://172.19.2.2:8501\n  External URL: http://34.55.202.18:8501\n\n\n--- Current Streamlit Log Tail (22:27:18) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:20) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:22) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:24) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:26) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:28) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:30) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:32) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:34) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:36) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:38) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:40) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:42) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:44) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:46) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:48) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:50) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:52) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:54) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:56) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:27:58) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:00) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:02) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:04) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:06) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:08) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:10) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:12) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:14) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:16) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:18) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:20) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:22) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:24) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:26) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:28) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:30) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:32) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:34) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:36) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:38) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:40) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:42) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:44) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:46) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:48) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:50) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:52) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:54) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:56) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:28:58) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:29:00) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:29:02) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:29:04) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:29:07) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:29:09) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:29:11) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:29:13) ---\n\n---------------------------------------\n\n\n--- Current Streamlit Log Tail (22:29:15) ---\n\n---------------------------------------\n\n\n--- Streamlit app did not report readiness within the expected time. ---\nIt might still be initializing or have encountered an early error. Ngrok may still fail.\nAttempting to create ngrok tunnel...\nngrok tunnel created successfully.\nYour ngrok URL is: NgrokTunnel: \"https://47c872b9171f.ngrok-free.app\" -> \"http://localhost:8501\"\n\nIf the URL shows 'ERR_NGROK_3200', the Streamlit app is likely still offline or crashed. Please check the Streamlit logs.\n\n--- IMPORTANT: Keep this Kaggle cell running to keep your Streamlit app alive! ---\nAccess your Face Unlock app at the ngrok URL: NgrokTunnel: \"https://47c872b9171f.ngrok-free.app\" -> \"http://localhost:8501\"\nTo terminate the app and free resources, manually press the 'Stop' button in Kaggle.\n","output_type":"stream"},{"name":"stderr","text":"2025-07-30 22:29:29.086875: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753914569.112935     238 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753914569.121092     238 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1753914569.143414     238 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1753914569.143439     238 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1753914569.143442     238 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1753914569.143444     238 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"/usr/lib/python3.11/threading.py\", line 1002, in _bootstrap\n    self._bootstrap_inner()\n  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 309, in _run_script_thread\n    self._run_script(request.rerun_data)\n  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 589, in _run_script\n    exec(code, module.__dict__)\n  File \"/kaggle/working/face_unlock_app/app.py\", line 5, in <module>\n    from utils import (\n  File \"/kaggle/working/face_unlock_app/utils.py\", line 5, in <module>\n    import tensorflow as tf\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\", line 468, in <module>\n    importlib.import_module(\"keras.src.optimizers\")\n  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"/usr/local/lib/python3.11/dist-packages/keras/__init__.py\", line 2, in <module>\n    from keras.api import DTypePolicy\n  File \"/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\", line 8, in <module>\n    from keras.api import activations\n  File \"/usr/local/lib/python3.11/dist-packages/keras/api/activations/__init__.py\", line 7, in <module>\n    from keras.src.activations import deserialize\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/__init__.py\", line 13, in <module>\n    from keras.src import visualization\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/__init__.py\", line 2, in <module>\n    from keras.src.visualization import plot_image_gallery\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_image_gallery.py\", line 13, in <module>\n    import matplotlib.pyplot as plt\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n    from . import _api, _version, cbook, _docstring, rcsetup\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n    from matplotlib.colors import Colormap, is_color_like\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n    from matplotlib import _api, _cm, cbook, scale\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n    from matplotlib.ticker import (\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n    from matplotlib import transforms as mtransforms\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n    from matplotlib._path import (\nAttributeError: _ARRAY_API not found\n\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"/usr/lib/python3.11/threading.py\", line 1002, in _bootstrap\n    self._bootstrap_inner()\n  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 309, in _run_script_thread\n    self._run_script(request.rerun_data)\n  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 589, in _run_script\n    exec(code, module.__dict__)\n  File \"/kaggle/working/face_unlock_app/app.py\", line 5, in <module>\n    from utils import (\n  File \"/kaggle/working/face_unlock_app/utils.py\", line 5, in <module>\n    import tensorflow as tf\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\", line 468, in <module>\n    importlib.import_module(\"keras.src.optimizers\")\n  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"/usr/local/lib/python3.11/dist-packages/keras/__init__.py\", line 2, in <module>\n    from keras.api import DTypePolicy\n  File \"/usr/local/lib/python3.11/dist-packages/keras/api/__init__.py\", line 34, in <module>\n    from keras.api import visualization\n  File \"/usr/local/lib/python3.11/dist-packages/keras/api/visualization/__init__.py\", line 11, in <module>\n    from keras.src.visualization.plot_bounding_box_gallery import (\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/visualization/plot_bounding_box_gallery.py\", line 12, in <module>\n    from matplotlib import patches  # For legend patches\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\", line 129, in <module>\n    from . import _api, _version, cbook, _docstring, rcsetup\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/rcsetup.py\", line 27, in <module>\n    from matplotlib.colors import Colormap, is_color_like\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py\", line 56, in <module>\n    from matplotlib import _api, _cm, cbook, scale\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/scale.py\", line 22, in <module>\n    from matplotlib.ticker import (\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/ticker.py\", line 138, in <module>\n    from matplotlib import transforms as mtransforms\n  File \"/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\", line 49, in <module>\n    from matplotlib._path import (\nAttributeError: _ARRAY_API not found\nI0000 00:00:1753914574.012312     238 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1753914574.016379     238 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_0.35_224_no_top.h5\n\u001b[1m2019640/2019640\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n[22:29:47] Kaggle cell active, Streamlit app should be running. Check ngrok URL.\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1753914600.483929     265 service.cc:152] XLA service 0x7ae2b0003720 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1753914600.483963     265 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1753914600.483969     265 service.cc:160]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1753914601.074950     265 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1753914605.036144     265 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"[22:30:17] Kaggle cell active, Streamlit app should be running. Check ngrok URL.\n[22:30:47] Kaggle cell active, Streamlit app should be running. Check ngrok URL.\n[22:31:17] Kaggle cell active, Streamlit app should be running. Check ngrok URL.\n[22:31:47] Kaggle cell active, Streamlit app should be running. Check ngrok URL.\n\nKaggle cell interrupted. Attempting cleanup...\nPerforming post-interruption cleanup...\nAdding missing 'registration_date' column to 'users' table.\n  Stopping...\n8501/tcp:              218\nCleanup commands executed.\nNo active ngrok tunnels found to kill.\nClean shutdown complete. Resources released.\n\n--- Full Streamlit Logs (after execution/interruption) ---\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}